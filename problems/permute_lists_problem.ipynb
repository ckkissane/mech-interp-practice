{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "collapsed_sections": [
        "JeENCT8b1kAv"
      ],
      "authorship_tag": "ABX9TyOPAOpLTZBG7/uLeYp8pGqm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem"
      ],
      "metadata": {
        "id": "PMeMP28yOQkT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpret a toy model trained to permute lists."
      ],
      "metadata": {
        "id": "NCx8qhgSORri"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeENCT8b1kAv"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ckkissane/mech-interp-practice.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-22Fksdq207j",
        "outputId": "bccef455-b08e-4aaa-e3cc-d0cdea8409a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mech-interp-practice'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 270 (delta 44), reused 26 (delta 9), pack-reused 181\u001b[K\n",
            "Receiving objects: 100% (270/270), 38.13 MiB | 7.28 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sxK_xiOO1kAv",
        "outputId": "8453680d-06ef-425c-f00c-1607e8fcb8bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-fm35mirf\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-fm35mirf\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 54d548de4995a1ecc5b01b9c03aceaf0966c0eb3\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n",
            "  Downloading jaxtyping-0.2.21-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.5.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.5.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.1)\n",
            "Collecting transformers>=4.25.1 (from transformer-lens==0.0.0)\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n",
            "  Downloading wandb-0.15.9-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.8.5)\n",
            "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (6.0.1)\n",
            "Collecting typeguard>=2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n",
            "  Downloading typeguard-4.1.3-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer-lens==0.0.0) (4.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->transformer-lens==0.0.0) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->transformer-lens==0.0.0) (2023.6.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.25.1->transformer-lens==0.0.0)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading sentry_sdk-1.30.0-py2.py3-none-any.whl (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: transformer-lens, pathtools\n",
            "  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=105997 sha256=a4f63d11481e6cdbba8e4606bf3e6e5062eff3a216a21ddfe9e8dc80982137cc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fwjpsesf/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e69bc24fa4ce3d05d869b7094be72847d58ae222bdd8d16265441e0d91045bd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built transformer-lens pathtools\n",
            "Installing collected packages: tokenizers, safetensors, pathtools, xxhash, typeguard, smmap, setproctitle, sentry-sdk, fancy-einsum, einops, docker-pycreds, dill, beartype, multiprocess, jaxtyping, huggingface-hub, gitdb, transformers, GitPython, wandb, datasets, transformer-lens\n",
            "Successfully installed GitPython-3.1.32 beartype-0.14.1 datasets-2.14.4 dill-0.3.7 docker-pycreds-0.4.0 einops-0.6.1 fancy-einsum-0.0.3 gitdb-4.0.10 huggingface-hub-0.16.4 jaxtyping-0.2.21 multiprocess-0.70.15 pathtools-0.1.2 safetensors-0.3.3 sentry-sdk-1.30.0 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformer-lens-0.0.0 transformers-4.32.1 typeguard-4.1.3 wandb-0.15.9 xxhash-3.3.0\n",
            "\n",
            "## Installing the NodeSource Node.js 16.x repo...\n",
            "\n",
            "\n",
            "## Populating apt-get cache...\n",
            "\n",
            "+ apt-get update\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Get:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [494 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [990 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [909 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [984 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,251 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,010 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,182 kB]\n",
            "Get:16 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:18 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main Sources [2,176 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy/main amd64 Packages [1,117 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [21.8 kB]\n",
            "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [37.3 kB]\n",
            "Fetched 10.6 MB in 7s (1,605 kB/s)\n",
            "Reading package lists... Done\n",
            "\n",
            "## Confirming \"jammy\" is supported...\n",
            "\n",
            "+ curl -sLf -o /dev/null 'https://deb.nodesource.com/node_16.x/dists/jammy/Release'\n",
            "\n",
            "## Adding the NodeSource signing key to your keyring...\n",
            "\n",
            "+ curl -s https://deb.nodesource.com/gpgkey/nodesource.gpg.key | gpg --dearmor | tee /usr/share/keyrings/nodesource.gpg >/dev/null\n",
            "\n",
            "## Creating apt sources list file for the NodeSource Node.js 16.x repo...\n",
            "\n",
            "+ echo 'deb [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' > /etc/apt/sources.list.d/nodesource.list\n",
            "+ echo 'deb-src [signed-by=/usr/share/keyrings/nodesource.gpg] https://deb.nodesource.com/node_16.x jammy main' >> /etc/apt/sources.list.d/nodesource.list\n",
            "\n",
            "## Running `apt-get update` for you...\n",
            "\n",
            "+ apt-get update\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:7 https://deb.nodesource.com/node_16.x jammy InRelease [4,583 B]\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:9 https://deb.nodesource.com/node_16.x jammy/main amd64 Packages [776 B]\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 5,359 B in 2s (2,808 B/s)\n",
            "Reading package lists... Done\n",
            "\n",
            "## Run `\u001b[1msudo apt-get install -y nodejs\u001b[m` to install Node.js 16.x and npm\n",
            "## You may also need development tools to build native addons:\n",
            "     sudo apt-get install gcc g++ make\n",
            "## To install the Yarn package manager, run:\n",
            "     curl -sL https://dl.yarnpkg.com/debian/pubkey.gpg | gpg --dearmor | sudo tee /usr/share/keyrings/yarnkey.gpg >/dev/null\n",
            "     echo \"deb [signed-by=/usr/share/keyrings/yarnkey.gpg] https://dl.yarnpkg.com/debian stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list\n",
            "     sudo apt-get update && sudo apt-get install yarn\n",
            "\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  nodejs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 27.2 MB of archives.\n",
            "After this operation, 128 MB of additional disk space will be used.\n",
            "Get:1 https://deb.nodesource.com/node_16.x jammy/main amd64 nodejs amd64 16.20.2-deb-1nodesource1 [27.2 MB]\n",
            "Fetched 27.2 MB in 1s (54.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nodejs.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../nodejs_16.20.2-deb-1nodesource1_amd64.deb ...\n",
            "Unpacking nodejs (16.20.2-deb-1nodesource1) ...\n",
            "Setting up nodejs (16.20.2-deb-1nodesource1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting git+https://github.com/neelnanda-io/PySvelte.git\n",
            "  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-0cd5oldd\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-0cd5oldd\n",
            "  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 6f5d971a148d40fb7481d400ae74551b37340e83\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (0.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (2.14.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.32.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from PySvelte==1.0.0) (1.5.3)\n",
            "Collecting typeguard~=2.0 (from PySvelte==1.0.0)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->PySvelte==1.0.0) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->PySvelte==1.0.0) (2023.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->PySvelte==1.0.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->PySvelte==1.0.0) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers->PySvelte==1.0.0) (0.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->PySvelte==1.0.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->PySvelte==1.0.0) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->PySvelte==1.0.0) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->PySvelte==1.0.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->PySvelte==1.0.0) (1.3.0)\n",
            "Building wheels for collected packages: PySvelte\n",
            "  Building wheel for PySvelte (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PySvelte: filename=PySvelte-1.0.0-py3-none-any.whl size=158305 sha256=8d7eb45dd32241a18030583a50e2c3102faf124ad9ff548c38d1ed7da9b90d0a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2dfbts_9/wheels/fa/f6/f2/673ef7aeb78d7503b6e3e42387132822fdc38d3ee283d3e5b4\n",
            "Successfully built PySvelte\n",
            "Installing collected packages: typeguard, PySvelte\n",
            "  Attempting uninstall: typeguard\n",
            "    Found existing installation: typeguard 4.1.3\n",
            "    Uninstalling typeguard-4.1.3:\n",
            "      Successfully uninstalled typeguard-4.1.3\n",
            "Successfully installed PySvelte-1.0.0 typeguard-2.13.3\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (2.13.3)\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEBUG_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "    # Needed for PySvelte to work, v3 came out and broke things...\n",
        "    %pip install typeguard==2.13.3\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ucSutS0c1kAw"
      },
      "outputs": [],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEBUG_MODE:\n",
        "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uR_n0Mrl1kAx"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YSZhMtBl1kAx"
      },
      "outputs": [],
      "source": [
        "import pysvelte\n",
        "\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPzBb_kI1kAy"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fBIDNqzV1kAy"
      },
      "outputs": [],
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "update_layout_set = {\"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\", \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\"}\n",
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    if isinstance(tensor, list):\n",
        "        tensor = torch.stack(tensor)\n",
        "    kwargs_post = {k: v for k, v in kwargs.items() if k in update_layout_set}\n",
        "    kwargs_pre = {k: v for k, v in kwargs.items() if k not in update_layout_set}\n",
        "    if \"facet_labels\" in kwargs_pre:\n",
        "        facet_labels = kwargs_pre.pop(\"facet_labels\")\n",
        "    else:\n",
        "        facet_labels = None\n",
        "    if \"color_continuous_scale\" not in kwargs_pre:\n",
        "        kwargs_pre[\"color_continuous_scale\"] = \"RdBu\"\n",
        "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0,labels={\"x\":xaxis, \"y\":yaxis}, **kwargs_pre).update_layout(**kwargs_post)\n",
        "    if facet_labels:\n",
        "        for i, label in enumerate(facet_labels):\n",
        "            fig.layout.annotations[i]['text'] = label\n",
        "\n",
        "    fig.show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(y=utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
        "\n",
        "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
        "    # Helper function to plot multiple lines\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    fig = go.Figure(layout={'title':title})\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = utils.to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    fig.show()\n",
        "\n",
        "def bar(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.bar(\n",
        "        y=utils.to_numpy(tensor),\n",
        "        labels={\"x\": xaxis, \"y\": yaxis},\n",
        "        template=\"simple_white\",\n",
        "        **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens.patching as patching\n",
        "from transformer_lens import evals\n",
        "import math"
      ],
      "metadata": {
        "id": "MzEwzeib1tjE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attn_patterns(heads, local_tokens, local_cache, title: str = \"\"):\n",
        "    labels = []\n",
        "    patterns = []\n",
        "    batch_index = 0\n",
        "\n",
        "    for head in heads:\n",
        "        if isinstance(head, tuple):\n",
        "            layer, head_index = head\n",
        "        else:\n",
        "            layer, head_index = head // model.cfg.n_heads, head % model.cfg.n_heads\n",
        "        patterns.append(local_cache[\"pattern\", layer][batch_index, head_index])\n",
        "        labels.append(f\"L{layer}H{head_index}\")\n",
        "    patterns = torch.stack(patterns, dim=-1)\n",
        "    attn_viz = pysvelte.AttentionMulti(tokens=model.to_str_tokens(local_tokens[batch_index]), attention=patterns, head_labels=labels)\n",
        "    display(HTML(f\"<h3>{title}</h3>\"))\n",
        "    attn_viz.show()"
      ],
      "metadata": {
        "id": "AgWNZo0c7sp3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "HdPsT2ZOxeua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "llysLeEq1kAy",
        "outputId": "cb6568f2-9c66-4eba-b9e0-18808ace4698",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7ee434a3a680>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "S0QibP7y7xCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c8bf18-74bf-4f53-bbd3-49a1728f888b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The smallest model I found was a 2L, 1H (per layer), attn-only transformer with no layernorm or biases. Note a 1L transformer was not able to solve this task (even with up to 8 heads).\n",
        "\n",
        "![picture](https://drive.google.com/uc?id=15DCokm0rjNEBFn5znNJ8mEu5ATF5z7CH)\n",
        "\n",
        "The model has already been trained, and is loaded into this notebook below:"
      ],
      "metadata": {
        "id": "z7_Jrzb9FWZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_LEN = 3\n",
        "MAX_INT = 50\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=2,\n",
        "    n_heads=1,\n",
        "    d_model=128,\n",
        "    d_head=128,\n",
        "    n_ctx=3*LIST_LEN+3, # BOS d1 d2 d3 MID p1 p2 p3 END a1 a2 a3\n",
        "    d_vocab= MAX_INT+3, # 0, ..., MAX_INT-1, BOS, MID, END\n",
        "    d_vocab_out=MAX_INT,\n",
        "    attn_only=True,\n",
        "    normalization_type=None,\n",
        "    device=device,\n",
        "    seed=0,\n",
        ")\n",
        "model = HookedTransformer(cfg)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_Qm7Gym7aJw",
        "outputId": "bdba0fca-0b8c-474d-fcd1-33bb651f7337"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HookedTransformer(\n",
            "  (embed): Embed()\n",
            "  (hook_embed): HookPoint()\n",
            "  (pos_embed): PosEmbed()\n",
            "  (hook_pos_embed): HookPoint()\n",
            "  (blocks): ModuleList(\n",
            "    (0-1): 2 x TransformerBlock(\n",
            "      (ln1): Identity()\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_pattern): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (hook_q_input): HookPoint()\n",
            "      (hook_k_input): HookPoint()\n",
            "      (hook_v_input): HookPoint()\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_in): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (unembed): Unembed()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"mech-interp-practice/models/permute_lists_model.pt\"\n",
        "state_dict = torch.load(filename)\n",
        "model.load_state_dict(state_dict, strict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZYaJ5Fn2_il",
        "outputId": "c12ce746-1e7f-4243-d348-32a3c71cbe41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task Description"
      ],
      "metadata": {
        "id": "ZxKuTab38hXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The models inputs are of the form:\n",
        "\n",
        "\n",
        "```\n",
        "[BOS, a_0, a_1, a_2, MID, idx_0, idx_1, idx_2, END, ans_0, ans_1, ans_2]\n",
        "```\n",
        "\n",
        "\n",
        "For example:\n",
        "\n",
        "\n",
        "```\n",
        "[BOS, 44, 40, 15, MID, 2, 1, 0, END, 15, 40, 44]\n",
        "```\n",
        "\n",
        "\n",
        "* BOS (52), MID (51), and END (50) are special tokens at the same position in every sequence\n",
        "* The first 3 tokens after BOS, [a_0, a_1, a_2], is the original list the model needs to permute. Each list token ranges between [0,49] inclusive.\n",
        "* The 3 tokens after MID, [idx_0, idx_1, idx_2] are the permutation indices. These are always some shuffling of {0, 1, 2}.\n",
        "* The last three tokens after END, [ans_0, ans_1, ans_2], is the permuted list after applying the perm indices. In general, ans_i = a_{idx_i}. In the concrete example above: ans_0 = a_{idx_0} = a_2 = 15.\n",
        "\n",
        "\n",
        "Note that the list length is fixed to 3 for every example. The model is trained to predict the ans tokens [ans_0, ans_1, ans_2] at positions [END, ans_0, ans_1] respectively. The model is trained with a causal mask so it can't peek ahead.\n",
        "\n",
        "\n",
        "Below I provide a data loader and some example tokens that you can use to start your investigation:"
      ],
      "metadata": {
        "id": "w--N4k3u3QNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data_generator(cfg, batch_size, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    BOS_TOKEN = cfg.d_vocab-1\n",
        "    MID_TOKEN = cfg.d_vocab-2\n",
        "    END_TOKEN = cfg.d_vocab-3\n",
        "    while True:\n",
        "        seq = torch.randint(0, MAX_INT, (batch_size, LIST_LEN))\n",
        "        perm = torch.randperm(LIST_LEN)\n",
        "        ans = seq[:, perm]\n",
        "\n",
        "        bos_tensor = einops.repeat(torch.tensor(BOS_TOKEN), \" -> i 1\", i=batch_size)\n",
        "        mid_tensor = einops.repeat(torch.tensor(MID_TOKEN), \" -> i 1\", i=batch_size)\n",
        "        end_tensor = einops.repeat(torch.tensor(END_TOKEN), \" -> i 1\", i=batch_size)\n",
        "\n",
        "        x = torch.cat([bos_tensor, seq, mid_tensor, einops.repeat(perm, \"seq -> batch seq\", batch=batch_size), end_tensor, ans], dim=-1)\n",
        "        yield x\n",
        "\n",
        "batch_size = 256\n",
        "data_loader = make_data_generator(cfg, batch_size, seed=42)\n",
        "\n",
        "test_data = []\n",
        "sub_batch_size = 4\n",
        "for i in range(batch_size // sub_batch_size):\n",
        "    test_data.append(next(make_data_generator(cfg, sub_batch_size, seed=i)))\n",
        "\n",
        "test_data = torch.cat(test_data, dim=0).to(device)\n",
        "print(test_data.shape)\n",
        "print(test_data[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34TX6ICq8fON",
        "outputId": "2afb0c95-31d9-4c25-fbf1-9dee41ae3449"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([256, 12])\n",
            "tensor([[52, 44, 39, 33, 51,  0,  2,  1, 50, 44, 33, 39],\n",
            "        [52, 10, 13, 29, 51,  0,  2,  1, 50, 10, 29, 13],\n",
            "        [52, 27,  3, 47, 51,  0,  2,  1, 50, 27, 47,  3],\n",
            "        [52, 33,  1, 16, 51,  0,  2,  1, 50, 33, 16,  1],\n",
            "        [52, 45, 39, 24, 51,  1,  2,  0, 50, 39, 24, 45]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model was trained to minimize cross entropy loss. I provide the loss function used in training below."
      ],
      "metadata": {
        "id": "g1tqxf3b3v61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(logits, tokens):\n",
        "    logits=logits[:, -LIST_LEN-1:-1, :]\n",
        "    logits = logits.to(torch.float64)\n",
        "    labels = tokens[:, -LIST_LEN:]\n",
        "\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[..., None])[..., 0]\n",
        "    return -correct_log_probs.mean()"
      ],
      "metadata": {
        "id": "dmQbkuI1_aOm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a helper function to convert tokens to a list of strings that you can use for plotly / CircuitsVis labels:"
      ],
      "metadata": {
        "id": "Tvu3U5XdnXNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_plotly_labels(tokens, incl_pos=True):\n",
        "    if tokens.ndim==2:\n",
        "        tokens = tokens[0]\n",
        "    if incl_pos:\n",
        "        res = [f\"{tok}_{i}\" for i, tok in enumerate(tokens)]\n",
        "    else:\n",
        "        res = list(map(str, tokens.tolist()))\n",
        "    res[0] = 'BOS'\n",
        "    res[LIST_LEN+1] = 'MID'\n",
        "    res[2*LIST_LEN+2]= 'END'\n",
        "    return res\n",
        "\n",
        "print(\"incl_pos=True:\", tokens_to_plotly_labels(test_data))\n",
        "print(\"incl_pos=False:\", tokens_to_plotly_labels(test_data, incl_pos=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcYnWBjlneo4",
        "outputId": "89e1dd31-3f5c-4ed4-bbf2-565ce68f66de"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "incl_pos=True: ['BOS', '44_1', '39_2', '33_3', 'MID', '0_5', '2_6', '1_7', 'END', '44_9', '33_10', '39_11']\n",
            "incl_pos=False: ['BOS', '44', '39', '33', 'MID', '0', '2', '1', 'END', '44', '33', '39']\n"
          ]
        }
      ]
    }
  ]
}